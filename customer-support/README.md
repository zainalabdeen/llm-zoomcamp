# ğŸ“š Customer Support RAG System

[![Docker Ready](https://img.shields.io/badge/Docker-Ready-blue.svg)]()
[![RAG Verified](https://img.shields.io/badge/RAG-Verified-success.svg)]()
[![LLM Evaluation](https://img.shields.io/badge/LLM-Evaluation-green.svg)]()
[![Retrieval Evaluation](https://img.shields.io/badge/Retrieval-Evaluation-orange.svg)]()
[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)]()
[![Deployable](https://img.shields.io/badge/Cloud-Deployable-lightgrey.svg)]()

---

## ğŸ§  Problem Description

Modern customer support teams are overwhelmed with repetitive FAQ queries, scattered documentation, and inconsistent responses. This project solves that by building a **Retrieval-Augmented Generation (RAG) system** that:

- Stores customer support FAQs in a semantic vector database.
- Retrieves accurate, relevant answers based on user queries.
- Generates improved answers using an LLM.
- Collects feedback and evaluates retrieval and generation quality.


---

## ğŸ“Š Dataset

- **Source:** [MakTek Customer Support FAQs](https://huggingface.co/datasets/MakTek/Customer_support_faqs_dataset)
- **Fields:** `question`, `answer`, `source`, `doc_id`

---

## ğŸ” End-to-End Pipeline

```
Data Ingestion â†’ Embedding â†’ Qdrant Vector DB
                     â†“
            BM25 Keyword Index
                     â†“
      Hybrid Retrieval + Reranking
                     â†“
     LLM Generation â†’ Feedback â†’ Dashboard
```

---

## ğŸ“¥ 1. Data Ingestion

We load and clean the MakTek FAQ dataset, deduplicate questions, convert them into `Document` objects, and index them automatically into Qdrant.

ğŸ“ Code: [`data_ingestion.py`](./data_ingestion.py)

---

## ğŸ§  2. Embedding & Storage

- **Embedding model:** [`intfloat/multilingual-e5-base`](https://huggingface.co/intfloat/multilingual-e5-base)
- **Vector DB:** [Qdrant](https://qdrant.tech/)
- **Framework:** [LlamaIndex](https://github.com/run-llama/llama_index)

Documents are converted into dense vectors and stored with metadata for efficient retrieval.

ğŸ“ Code: [`data_ingestion.py`](./data_ingestion.py)

---

## ğŸ” 3. Retrieval Pipeline


| Stage | Method | Purpose |
|-------|--------|---------|
| ğŸ” Vector Search | Dense retrieval via LlamaIndex + Qdrant | Semantic similarity |
| ğŸ“š BM25 | Keyword retrieval | Text relevance |
| ğŸ§  Hybrid | Combines vector + BM25 | **[Best Practice âœ…]** |
| ğŸ“Š Reranking | `cross-encoder/ms-marco-MiniLM-L-6-v2` | **[Best Practice âœ…]** |

ğŸ“ Code: [`search_process.py`](./search_process.py)

---

## ğŸ¤– 4. LLM Integration

- **Models:** `gpt-4o-mini`, `gpt-3.5-turbo`
- Uses retrieved context to generate final answers
- Handles â€œno-answerâ€ cases gracefully

ğŸ“ Code: [`search_process.py`](./search_process.py)

---

## ğŸ–¥ï¸ 5. User Interface


- **User App:** Semantic search interface with optional LLM answers  
- **Admin Dashboard:** Feedback analytics + retrieval evaluation visualizations  
- **Password Protection:** Only authorized users can access the dashboard  

ğŸ“ Code: [`frontend/app.py`](./frontend/app.py), [`frontend/dashboard.py`](./frontend/dashboard.py)

ğŸ–¼ï¸ Screenshots:  
- ![User App](./images/app1.png)  
- ![Dashboard](./images/app2.png)

---

## ğŸ“ˆ 6. Evaluation

### ğŸ“Š Retrieval Evaluation


We compare:

1. Dense Vector Search  
2. Hybrid Search  
3. Hybrid + Reranker  

Metrics: **Hit Rate@5**, **MRR@5**

ğŸ“ Code: [`search_evaluation.ipynb`](./notebook/search_evaluation.ipynb)  
ğŸ–¼ï¸ ![Retrieval Evaluation](./images/search_evaluation.png)

Based on a generated ground truth dataset of questions,i applied multi metrics to evalute the Dense Vector Search,Hybrid Search and Hybrid + Reranker.
Hybrid + Reranker demonstrated superior performance and was chosen as the preferred method.
---

### ğŸ¤– LLM Evaluation


We evaluate `gpt-4o-mini` and `gpt-3.5-turbo` answers vs. ground truth using cosine similarity.

ğŸ“ Code: [`llm_evaluation.ipynb`](./notebook/llm_evaluation.ipynb)  
ğŸ–¼ï¸ ![LLM Evaluation](./images/rag_evaluation.png)
ğŸ–¼ï¸ ![LLM Evaluation](./images/rag_evaluation2.png)

Relevance results were show as the gpt-3.5-turbo is better so we selected it as the default model.
---

## ğŸ“Š 7. Monitoring & Feedback


- Collect user feedback (text + rating)
- Store it in PostgreSQL
- Analyze with charts: satisfaction trends, query volume, top queries, etc.

ğŸ“ Code: [`backend/feedback_service.py`](./backend/feedback_service.py)

---

## ğŸ³ 8. Containerization


- Qdrant + PostgreSQL + RAG App in one command
- Reproducible and cloud-ready

ğŸ“ Code: [`Dockerfile`](./Dockerfile), [`docker-compose.yml`](./docker-compose.yml)

---

## ğŸ“ Project Structure

```
customer-support-rag/
â”œâ”€ backend/
â”‚   â”œâ”€ rag_service.py
â”‚   â””â”€ feedback_service.py
â”œâ”€ frontend/
â”‚   â”œâ”€ app.py
â”‚   â””â”€ dashboard.py
â”œâ”€ data_ingestion.py
â”œâ”€ search_process.py
â”œâ”€ notebook/
â”‚   â”œâ”€ search_evaluation.py
â”‚   â””â”€ llm_evaluation.py
â”œâ”€ main.py
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â””â”€ requirements.txt
```

---

## ğŸ§ª How to Run

### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/zainalabdeen/llm-zoomcamp/customer-support.git
cd customer-support
```

### 2ï¸âƒ£ Set environment variables
Create a `.env` file:

```
OPENAI_API_KEY=sk-xxxxxx
ADMIN_PASSWORD=SuperSecret
```

### 3ï¸âƒ£ Launch with Docker Compose
```bash
docker-compose up --build
```

âœ… Access:  
- User App â†’ http://localhost:8501  
- Admin Dashboard â†’ http://localhost:8502

---

## ğŸ“œ License

MIT License Â© 2025 â€“ Developed by **ZainAlabdeen**  
