{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4dd0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "# LlamaIndex core\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    Document,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "# Nodes & postprocessors\n",
    "from llama_index.core.schema import TextNode, NodeWithScore\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61460b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://localhost:6333\")\n",
    "QDRANT_COLLECTION = os.getenv(\"QDRANT_COLLECTION\", \"maktek_faqs\")\n",
    "EMBED_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "RERANK_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "K = 5  # we’ll evaluate @5 as requested\n",
    "# knobs\n",
    "VEC_TOP_K = max(12, K)      # gather more for better recall\n",
    "BM25_TOP_K = max(12, K)\n",
    "RERANK_TOP_N = max(8, K)\n",
    "FINAL_TOP_N = K  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7139af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CorpusItem:\n",
    "    doc_id: str\n",
    "    text: str  \n",
    "    metadata: Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710fa6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# DATA LOADING\n",
    "# -----------------------------\n",
    "def load_maktek_dataset() -> List[Document]:\n",
    "    #ds = load_dataset(\"MakTek/Customer_support_faqs_dataset\", split=\"train\")\n",
    "    df = pd.read_csv('../data/data.csv')\n",
    "    df_dict = df.to_dict(orient='records')\n",
    "    docs: List[Document] = []\n",
    "    for i, row in enumerate(df_dict):\n",
    "        q = (row.get(\"question\") or \"\").strip()\n",
    "        a = (row.get(\"answer\") or \"\").strip()\n",
    "        doc_id = f\"faq-{i:04d}\"\n",
    "        text = f\"Q: {q}\\n\\nA: {a}\"\n",
    "        metadata = {\"question\": q, \"answer\": a, \"source\": \"MakTek\", \"doc_id\": doc_id}\n",
    "        docs.append(Document(text=text, metadata=metadata, doc_id=doc_id))\n",
    "    return docs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c07010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Connect to Qdrant collection\n",
    "# --------------------------\n",
    "def connect_to_index() -> VectorStoreIndex:\n",
    "    embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)\n",
    "    client = QdrantClient(url=QDRANT_URL)\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=QDRANT_COLLECTION)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    return VectorStoreIndex.from_vector_store(vector_store=vector_store,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a0bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# BM25 from metadata (optional, if needed)\n",
    "# --------------------------\n",
    "def simple_tokenize(s: str) -> List[str]:\n",
    "    return s.lower().split()\n",
    "\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25, corpus_items\n",
    "\n",
    "def build_bm25_corpus(docs: List[Document]) -> Tuple[BM25Okapi, Dict[str, CorpusItem]]:\n",
    "    corpus_items: Dict[str, CorpusItem] = {}\n",
    "    tokenized_corpus = []\n",
    "    for d in docs:\n",
    "        doc_id = d.doc_id or d.metadata.get(\"doc_id\")\n",
    "        corpus_items[doc_id] = CorpusItem(doc_id=doc_id, text=d.text, metadata=d.metadata)\n",
    "        tokenized_corpus.append(simple_tokenize(d.text))\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25, corpus_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc5b8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# RETRIEVAL HELPERS\n",
    "# -----------------------------\n",
    "def bm25_retrieve(bm25: BM25Okapi, corpus_items: Dict[str, CorpusItem], query: str, top_k: int) -> List[NodeWithScore]:\n",
    "    scores = bm25.get_scores(simple_tokenize(query))\n",
    "    ranked = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    doc_ids = list(corpus_items.keys())\n",
    "    results: List[NodeWithScore] = []\n",
    "    for idx, sc in ranked:\n",
    "        doc_id = doc_ids[idx]\n",
    "        item = corpus_items[doc_id]\n",
    "        node = TextNode(id_=doc_id, text=item.text, metadata=item.metadata)\n",
    "        results.append(NodeWithScore(node=node, score=float(sc)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79175df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_retrieve(index: VectorStoreIndex, query: str, top_k: int) -> List[NodeWithScore]:\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "    return retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b072db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Reranker builder\n",
    "# --------------------------\n",
    "def build_reranker() -> SentenceTransformerRerank:\n",
    "    return SentenceTransformerRerank(model=RERANK_MODEL_NAME, top_n=RERANK_TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19537787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_results(nodes: List[NodeWithScore]) -> List[Dict]:\n",
    "    \"\"\"Convert NodeWithScore → dict rows.\"\"\"\n",
    "    out = []\n",
    "    for n in nodes[:FINAL_TOP_N]:\n",
    "        out.append({\n",
    "            \"doc_id\": n.metadata.get(\"doc_id\") if n.metadata else n.node.node_id,\n",
    "            \"question\": n.metadata.get(\"question\", \"\"),\n",
    "            \"answer\": n.metadata.get(\"answer\", \"\"),\n",
    "            \"score\": float(n.score or 0.0),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def dedup_exact_question(rows: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Keep only the first instance of identical question text (case-insensitive).\"\"\"\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for r in rows:\n",
    "        key = (r.get(\"question\") or \"\").strip().lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(r)\n",
    "        if len(uniq) >= FINAL_TOP_N:\n",
    "            break\n",
    "    return uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb3c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Query (Retrieval Only)\n",
    "# --------------------------\n",
    "def query_without_llm(index: VectorStoreIndex, query: str) -> List[Dict]:\n",
    "    retriever = index.as_retriever(similarity_top_k=VEC_TOP_K)\n",
    "    results = retriever.retrieve(query)\n",
    "    reranker = build_reranker()\n",
    "    reranked = reranker.postprocess_nodes(results, query_str=query)\n",
    "\n",
    "    # Deduplicate by question text\n",
    "    seen_questions = set()\n",
    "    final_results = []\n",
    "    for n in reranked:\n",
    "        q = (n.metadata.get(\"question\") or \"\").strip().lower()\n",
    "        if q not in seen_questions:\n",
    "            seen_questions.add(q)\n",
    "            final_results.append({\n",
    "                \"doc_id\": n.metadata.get(\"doc_id\"),\n",
    "                \"question\": n.metadata.get(\"question\"),\n",
    "                \"answer\": n.metadata.get(\"answer\"),\n",
    "                \"score\": n.score\n",
    "            })\n",
    "        if len(final_results) >= FINAL_TOP_N:\n",
    "            break\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ee80df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Query (With LLM)\n",
    "# --------------------------\n",
    "def query_with_llm(index: VectorStoreIndex, query: str) -> Dict:\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise RuntimeError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "    results = query_without_llm(index, query)\n",
    "    context_text = \"\\n\\n\".join([f\"[{i+1}] Q: {r['question']}\\nA: {r['answer']}\" for i, r in enumerate(results)])\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful support assistant. Answer the user's question using ONLY the context below.\n",
    "If the answer is not present, say you don't have enough information.\n",
    "\n",
    "User question:\n",
    "{query}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Return a concise, direct answer.\n",
    "\"\"\"\n",
    "\n",
    "    llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    completion = llm.complete(prompt)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": completion.text.strip(),\n",
    "        \"top_context\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8505503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_search() -> Dict:\n",
    "    docs = load_maktek_dataset()\n",
    "    index = connect_to_index()\n",
    "    bm25, corpus_items = build_bm25_corpus(docs)\n",
    "    return {'docs': docs ,'index':index,'bm25':bm25,'corpus_items':corpus_items}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
